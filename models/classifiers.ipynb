{"cells":[{"cell_type":"code","execution_count":22,"id":"IDtz-Nel-7E0","metadata":{"executionInfo":{"elapsed":236,"status":"ok","timestamp":1700932464962,"user":{"displayName":"Monsieur Cacao","userId":"17438377351391691459"},"user_tz":-60},"id":"IDtz-Nel-7E0"},"outputs":[],"source":["# GENERAL CONFIGURATION ========================================================\n","\n","# if use of google colab, do not forget to set the path\n","use_google_colab = False\n","\n","# set a debug mode\n","debug = False\n","\n","\n","#set if we want to clean the or load the precleaned data\n","clean_data_again = True\n","\n","\n","# select if we want to order the vocabulary by frequency in the text [needed for the calculation of the weight]\n","reorder_vocabulary = True\n","# Select if we want to get the list of the words present in the text but not in the vocabulary\n","list_words_not_in_vocab = True\n","# choose pooling method\n","pooling_method = \"x\" # \"mean\", \"max\", \"tfidf\", \"weight\"\n","# choose technique to use\n","model_type = \"x\" # \"logistic\", \"svm\", \"neural_net\"\n","\n","if use_google_colab :\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  %cd /content/drive/MyDrive/ColabNotebooks/sentiMentaL_tweets\n","# check coherence of the settings ====================================\n"]},{"cell_type":"code","execution_count":23,"id":"efIRCfG22AP7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1550,"status":"ok","timestamp":1700932466967,"user":{"displayName":"Monsieur Cacao","userId":"17438377351391691459"},"user_tz":-60},"id":"efIRCfG22AP7","outputId":"82ec3562-63bf-491e-e225-c4925b2e5eb4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Warning: debug mode is on and clean_data_again has been reset to False.\n"]}],"source":["\n","\n","if debug and clean_data_again:\n","  #clean_data_again = False\n","  print(\"Warning: debug mode is on and clean_data_again has been reset to False.\")\n","  \n","if not reorder_vocabulary and pooling_method == \"weight\":\n","  reorder_vocabulary = True\n","  print(\"Warning: reorder_vocabulary has been reset to True. Because pooling_method is set to weight and needs the vocabulary to be ordered by frequency in the text.\")\n"]},{"cell_type":"code","execution_count":24,"id":"22e8a0bc","metadata":{"executionInfo":{"elapsed":1002,"status":"ok","timestamp":1700932467967,"user":{"displayName":"Monsieur Cacao","userId":"17438377351391691459"},"user_tz":-60},"id":"22e8a0bc"},"outputs":[],"source":["# Imports ---------------------------------------------------------------------\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","import pandas as pd\n","\n","import sys\n","import os\n","\n","# get scikit-learn and scaler\n","import sklearn\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import SGDClassifier\n","from keras.models import Sequential\n","from keras.layers import Dense\n","\n","\n","# Create an instance of StandardScaler\n","scaler = StandardScaler()\n","\n","# Get the current working directory\n","current_dir = os.getcwd()\n","\n","# Adjust the path to point to the parent directory\n","parent_dir = os.path.dirname(current_dir)\n","\n","# Add the parent directory to sys.path\n","sys.path.insert(0, parent_dir)\n","\n","import helpers.preprocessing_helper as pre\n","import helpers.classifiers_helper as clas\n","import helpers.submission_helper as sub\n","\n","# Load data -------------------------------------------------------------------"]},{"cell_type":"code","execution_count":25,"id":"8c857efb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1700932467967,"user":{"displayName":"Monsieur Cacao","userId":"17438377351391691459"},"user_tz":-60},"id":"8c857efb","outputId":"1f4ecff4-8ce3-49e2-d134-261a022b95e9"},"outputs":[],"source":["# Load the word embeddings\n","word_embeddings = np.load('word_embeddings/embeddings.npy')\n","df_word_embeddings = pd.DataFrame(word_embeddings)\n","\n"]},{"cell_type":"code","execution_count":26,"id":"8955ce2c","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1700932467967,"user":{"displayName":"Monsieur Cacao","userId":"17438377351391691459"},"user_tz":-60},"id":"8955ce2c","outputId":"ef2df9d8-80ff-4920-cb44-e20884096840"},"outputs":[],"source":["# Load the test set tweets\n","with open('../twitter-datasets/test_data.txt', 'r', encoding='utf-8') as file:\n","    test_tweets = file.readlines()\n","    df_test_tweets = pd.DataFrame(test_tweets)\n","\n"]},{"cell_type":"code","execution_count":27,"id":"157abb4e","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1700932467968,"user":{"displayName":"Monsieur Cacao","userId":"17438377351391691459"},"user_tz":-60},"id":"157abb4e","outputId":"447bb399-fc01-4b01-cb9a-c9c113eaf2e0"},"outputs":[],"source":["# Load the vocabulary\n","with open('word_embeddings/processed_vocab_cut.txt', 'r', encoding='utf-8') as file:\n","    vocabulary = file.read().splitlines()\n"]},{"cell_type":"code","execution_count":28,"id":"7ecb2448","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":495,"status":"ok","timestamp":1700932469862,"user":{"displayName":"Monsieur Cacao","userId":"17438377351391691459"},"user_tz":-60},"id":"7ecb2448","outputId":"4a025ad9-997d-4926-ecea-78582b7488c9"},"outputs":[],"source":["# Load positive training tweets and assign labels\n","with open('../twitter-datasets/train_pos_full.txt', 'r', encoding='utf-8') as file:\n","    pos_tweets = file.readlines()\n","\n"]},{"cell_type":"code","execution_count":29,"id":"c7de2422","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":315,"status":"ok","timestamp":1700932470175,"user":{"displayName":"Monsieur Cacao","userId":"17438377351391691459"},"user_tz":-60},"id":"c7de2422","outputId":"4c074839-57cd-46a4-f332-ad3cec1dd1e9"},"outputs":[],"source":["# Load negative training tweets and assign labels\n","with open('../twitter-datasets/train_neg_full.txt', 'r', encoding='utf-8') as file:\n","    neg_tweets = file.readlines()\n","\n","\n"]},{"cell_type":"markdown","id":"b9050c43","metadata":{},"source":["#### End of the loading"]},{"cell_type":"markdown","id":"08157977","metadata":{"id":"08157977"},"source":["___________________________"]},{"cell_type":"code","execution_count":30,"id":"e56499c3","metadata":{"executionInfo":{"elapsed":266,"status":"ok","timestamp":1700932584534,"user":{"displayName":"Monsieur Cacao","userId":"17438377351391691459"},"user_tz":-60},"id":"e56499c3"},"outputs":[],"source":["# Clean data ------------------------------------------------------------------\n","if debug:\n","    pos_tweets = pos_tweets[:10]\n","    neg_tweets = neg_tweets[:10]\n","    test_tweets = test_tweets[:10]\n","    #all_tweets = np.concatenate((train_tweets, test_tweets), axis=0)\n","\n","    vocabulary = vocabulary[:100]"]},{"cell_type":"code","execution_count":31,"id":"0ca846e9","metadata":{},"outputs":[],"source":["def cleaning_text(data):\n","    data = pre.drop_duplicates(data)\n","    data = pre.remove_elongs(data)\n","    data = pre.lower_case(data)\n","    data = pre.spell_correct(data)\n","    data = pre.lemmatizer(data)\n","    data = pre.stopword(data)\n","    data = pre.hashtag(data)\n","    return data"]},{"cell_type":"code","execution_count":32,"id":"cfdea12c","metadata":{},"outputs":[],"source":["# Transform the data to reuse the code from someone else\n","pos_labels = np.ones(len(pos_tweets), dtype=int)  # Assign label 1 for positive tweets\n","neg_labels = -1 * np.ones(len(neg_tweets), dtype=int)  # Assign label -1 for negative tweets\n","\n","pos_tweets = pd.DataFrame({'text': pos_tweets, 'label': pos_labels})\n","neg_tweets = pd.DataFrame({'text': neg_tweets, 'label': neg_labels})\n","ids = np.arange(len(test_tweets))+1\n","\n","test_tweets = pd.DataFrame({'ids' : ids, 'text': test_tweets})"]},{"cell_type":"code","execution_count":33,"id":"b3d8fa79","metadata":{},"outputs":[],"source":["# Use our cleaning function\n","pos_tweets = cleaning_text(pos_tweets)\n","neg_tweets = cleaning_text(neg_tweets)\n","test_tweets = cleaning_text(test_tweets)"]},{"cell_type":"code","execution_count":34,"id":"11c9a1e6","metadata":{},"outputs":[],"source":["# from pd dataframe to np array\n","pos_tweets = np.array(pos_tweets['text'])\n","neg_tweets = np.array(neg_tweets['text'])\n","test_tweets = np.array(test_tweets['text'])"]},{"cell_type":"code","execution_count":35,"id":"MhFIFQUVMjAf","metadata":{"executionInfo":{"elapsed":11114,"status":"ok","timestamp":1700932493637,"user":{"displayName":"Monsieur Cacao","userId":"17438377351391691459"},"user_tz":-60},"id":"MhFIFQUVMjAf"},"outputs":[],"source":["#reorder the vocabulary and the word embeddings according to the largest number of occurences first\n","if reorder_vocabulary :\n","    vocabulary, word_to_embeddings = clas.reorder_vocabulary(pos_tweets, neg_tweets, test_tweets, vocabulary, word_embeddings, clean_data_again, save_counts=True)"]},{"cell_type":"code","execution_count":36,"id":"a3f48ba4","metadata":{},"outputs":[],"source":["# Convert the values into a NumPy array\n","word_embeddings = np.array(list(word_to_embeddings.values()))"]},{"cell_type":"code","execution_count":37,"id":"b902273c","metadata":{},"outputs":[],"source":["# Create a dictionary to map words to their corresponding embeddings\n","word_to_embedding = {word: word_embeddings[i] for i, word in enumerate(vocabulary)}\n"]},{"cell_type":"code","execution_count":38,"id":"90bc1acc","metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1700932132486,"user":{"displayName":"Monsieur Cacao","userId":"17438377351391691459"},"user_tz":-60},"id":"90bc1acc"},"outputs":[],"source":["if list_words_not_in_vocab :\n","    #save the words that are not in the vocabulary\n","    clas.out_of_vocab_file(pos_tweets, neg_tweets, test_tweets, vocabulary, clean_data_again)"]},{"cell_type":"code","execution_count":39,"id":"35a2a9f9","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"executionInfo":{"elapsed":309039,"status":"error","timestamp":1700932442215,"user":{"displayName":"Monsieur Cacao","userId":"17438377351391691459"},"user_tz":-60},"id":"35a2a9f9","outputId":"651554fb-26e7-474b-be5c-b2dbca5668a6"},"outputs":[{"ename":"ValueError","evalue":"Pooling method not recognized","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[1;32mc:\\Users\\gross\\Desktop\\sentiMentaL_tweets\\models\\regression - Copy.ipynb Cell 20\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gross/Desktop/sentiMentaL_tweets/models/regression%20-%20Copy.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m### TRAINING THE  CLASSIFIER\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/gross/Desktop/sentiMentaL_tweets/models/regression%20-%20Copy.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m train_features, test_features \u001b[39m=\u001b[39m clas\u001b[39m.\u001b[39;49mget_features(pooling_method, pos_tweets, neg_tweets, test_tweets, word_embeddings, vocabulary, clean_data_again)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gross/Desktop/sentiMentaL_tweets/models/regression%20-%20Copy.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Split the data into training and validation sets\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/gross/Desktop/sentiMentaL_tweets/models/regression%20-%20Copy.ipynb#X25sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((pos_labels, neg_labels), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n","File \u001b[1;32mc:\\Users\\gross\\Desktop\\sentiMentaL_tweets\\helpers\\classifiers_helper.py:119\u001b[0m, in \u001b[0;36mget_features\u001b[1;34m(pooling_method, train_tweets_pos, train_tweets_neg, test_tweets, word_to_embedding, vocabulary, clean_data_again)\u001b[0m\n\u001b[0;32m    115\u001b[0m     test_features \u001b[39m=\u001b[39m all_features[\u001b[39mlen\u001b[39m(train_tweets):]\n\u001b[0;32m    118\u001b[0m \u001b[39melse\u001b[39;00m : \n\u001b[1;32m--> 119\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPooling method not recognized\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    121\u001b[0m \u001b[39mreturn\u001b[39;00m train_features, test_features\n","\u001b[1;31mValueError\u001b[0m: Pooling method not recognized"]}],"source":["### TRAINING THE  CLASSIFIER\n","\n","\n","train_features, test_features = clas.get_features(pooling_method, pos_tweets, neg_tweets, test_tweets, word_embeddings, vocabulary, clean_data_again)\n","# Split the data into training and validation sets\n","labels = np.concatenate((pos_labels, neg_labels), axis=0)\n","\n","\n","train_features = np.array(train_features)\n","labels = np.array(labels)\n","# Assuming train_features and labels are NumPy arrays\n","assert len(train_features) == len(labels), \"Features and labels must be of the same length\"\n","\n","# Generate a permutation of indices\n","shuffled_indices = np.random.permutation(len(train_features))\n","\n","# Apply the shuffled indices to both features and labels\n","shuffled_features = train_features[shuffled_indices]\n","shuffled_labels = labels[shuffled_indices]\n","\n","\n","\n","X_train, X_val, y_train, y_val = train_test_split(train_features, labels, test_size=0.1, random_state=42)\n","\n","if model_type == \"logistic\":\n","    # Initialize and train the model\n","    model = LogisticRegression()\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_val)\n","\n","\n","elif model_type == \"svm\":\n","    X_train = scaler.fit_transform(X_train)\n","    X_test = scaler.transform(X_val)\n","\n","    # Create an SGDClassifier with a linear SVM loss\n","    model = SGDClassifier(loss='hinge', alpha=0.0001, max_iter=100, random_state=42, learning_rate='optimal', eta0=0.0, early_stopping=True, n_iter_no_change=5)\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_val)\n","\n","\n","elif model_type == \"neural_net\":\n","\n","    # Create and compile your Keras model\n","    model = Sequential()\n","    model.add(Dense(100, input_dim=X_train.shape[1], activation='relu'))\n","    model.add(Dense(1, activation='sigmoid'))\n","    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    # Fit the model\n","    model.fit(X_train, y_train, epochs=25, batch_size=64, validation_split=0.1)\n","    \n","    # Make predictions on your test data\n","    y_pred = (model.predict(X_val) > 0.5).astype(\"int32\")\n","    \n","else:\n","    raise ValueError(\"model_type is not recognized\")\n","\n","\n","\n","# Validate\n","accuracy = accuracy_score(y_val, y_pred)\n","print(f\"Validation Accuracy: {accuracy}\")"]},{"cell_type":"code","execution_count":null,"id":"0fe11ed8","metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1700932442216,"user":{"displayName":"Monsieur Cacao","userId":"17438377351391691459"},"user_tz":-60},"id":"0fe11ed8"},"outputs":[],"source":["###  PREDICTIONS\n","\n","# Construct feature representations for test tweets\n","test_features = np.array(test_features)\n","# Make predictions\n","y_test_pred = model.predict(test_features)\n","\n","test_data_path = \"../twitter-datasets/test_data.txt\"\n","ids_test = sub.get_test_ids(test_data_path)\n","y_pred = []\n","y_pred = y_test_pred\n","y_pred[y_pred <= 0] = -1\n","y_pred[y_pred > 0] = 1\n","y_pred = y_pred.astype(int)\n","print(y_pred)\n","sub.create_csv_submission(ids_test, y_pred, \"../submissions/submission_\"+pooling_method+\"_\"+model_type+\".csv\")"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":5}
