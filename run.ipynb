{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22e8a0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### IMPORTS AND CONSTANTS\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# !pip install scikit-learn\n",
    "\n",
    "test_data_path = 'twitter-datasets/test_data.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c15efa54",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATA LOADING\n",
    "\n",
    "# Load the word embeddings\n",
    "word_embeddings = np.load('embeddings.npy')\n",
    "\n",
    "# Load the test set tweets\n",
    "with open('twitter-datasets/test_data.txt', 'r', encoding='utf-8') as file:\n",
    "    test_tweets = file.readlines()\n",
    "\n",
    "# Load the vocabulary\n",
    "with open('vocab_cut.txt', 'r', encoding='utf-8') as file:\n",
    "    vocabulary = file.read().splitlines()\n",
    "\n",
    "# Create a dictionary to map words to their corresponding embeddings\n",
    "word_to_embedding = {word: word_embeddings[i] for i, word in enumerate(vocabulary)}\n",
    "\n",
    "# Load positive training tweets and assign labels\n",
    "with open('twitter-datasets/train_pos_full.txt', 'r', encoding='utf-8') as file:\n",
    "    pos_tweets = file.readlines()\n",
    "\n",
    "pos_labels = np.ones(len(pos_tweets), dtype=int)  # Assign label 1 for positive tweets\n",
    "\n",
    "# Load negative training tweets and assign labels\n",
    "with open('twitter-datasets/train_neg_full.txt', 'r', encoding='utf-8') as file:\n",
    "    neg_tweets = file.readlines()\n",
    "\n",
    "neg_labels = -1 * np.ones(len(neg_tweets), dtype=int)  # Assign label -1 for negative tweets\n",
    "\n",
    "# Combine positive and negative tweets and labels\n",
    "train_tweets = pos_tweets + neg_tweets\n",
    "labels = np.concatenate((pos_labels, neg_labels), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddf4a178",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE FUNCTIONS\n",
    "\n",
    "def average_word_vectors(tweet, word_to_embedding):\n",
    "    words = tweet.split()\n",
    "    vectors = [word_to_embedding[word] for word in words if word in word_to_embedding]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        # If none of the words in the tweet are in the embeddings, return a zero vector\n",
    "        return np.zeros_like(word_embeddings[0])\n",
    "    \n",
    "def create_csv_submission(ids, y_pred, name):\n",
    "    \"\"\"\n",
    "    This function creates a csv file named 'name' in the format required for a submission in Kaggle or AIcrowd.\n",
    "    The file will contain two columns the first with 'ids' and the second with 'y_pred'.\n",
    "    y_pred must be a list or np.array of 1 and -1 otherwise the function will raise a ValueError.\n",
    "\n",
    "    Args:\n",
    "        ids (list,np.array): indices\n",
    "        y_pred (list,np.array): predictions on data correspondent to indices\n",
    "        name (str): name of the file to be created\n",
    "    \"\"\"\n",
    "    # Check that y_pred only contains -1 and 1\n",
    "    if not all(i in [-1, 1] for i in y_pred):\n",
    "        raise ValueError(\"y_pred can only contain values -1, 1\")\n",
    "\n",
    "    with open(name, \"w\", newline=\"\") as csvfile:\n",
    "        fieldnames = [\"Id\", \"Prediction\"]\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({\"Id\": int(r1), \"Prediction\": int(r2)})\n",
    "\n",
    "def get_test_ids(path):\n",
    "    file = open(path,'r')\n",
    "    lines = file.readlines()\n",
    "    for rowidx in range(len(lines)):\n",
    "        index = lines[rowidx].index(',')\n",
    "        lines[rowidx] = lines[rowidx][:index]\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35a2a9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.577728\n"
     ]
    }
   ],
   "source": [
    "### TRAINING THE LINEAR CLASSIFIER\n",
    "\n",
    "# Construct feature representations for training tweets\n",
    "train_features = [average_word_vectors(tweet, word_to_embedding) for tweet in train_tweets]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_features, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Validate\n",
    "y_pred = model.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0fe11ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1  1 ... -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "### LINEAR CLASSIFIER PREDICTIONS\n",
    "\n",
    "# Construct feature representations for test tweets\n",
    "test_features = [average_word_vectors(tweet, word_to_embedding) for tweet in test_tweets]\n",
    "\n",
    "# Make predictions\n",
    "y_test_pred = model.predict(test_features)\n",
    "\n",
    "\n",
    "\n",
    "ids_test = get_test_ids(test_data_path)\n",
    "print(y_test_pred)\n",
    "y_pred = []\n",
    "y_pred = y_test_pred\n",
    "y_pred[y_pred <= 0] = -1\n",
    "y_pred[y_pred > 0] = 1\n",
    "create_csv_submission(ids_test, y_pred, \"submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef086dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
